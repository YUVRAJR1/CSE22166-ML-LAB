{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3d65fb1-8ee7-434e-8997-3ae21d59266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function for the summation unit\n",
    "def summation_unit(inputs, weights):\n",
    "    return np.dot(inputs, weights)\n",
    "\n",
    "# Activation Functions\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def bipolar_step_function(x):\n",
    "    return 1 if x >= 0 else -1\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh_function(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu_function(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def leaky_relu_function(x, alpha=0.01):\n",
    "    return x if x >= 0 else alpha * x\n",
    "\n",
    "# Error Comparator Unit\n",
    "def comparator_unit(predicted, actual):\n",
    "    return predicted - actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d65430ed-f41e-4bf2-abac-ea1b2f4d9be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights after training: [160.    50.2   49.25]\n",
      "Number of epochs taken to converge: 1000\n"
     ]
    }
   ],
   "source": [
    "#A2\n",
    "\n",
    "# Training data for AND Gate\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([0, 0, 0, 1])  # AND Gate outputs\n",
    "weights = np.array([10, 0.2, -0.75])  # Including bias as weight\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Activation function - Step function\n",
    "def perceptron_training(inputs, outputs, weights, learning_rate, max_epochs=1000, error_threshold=0.002):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    while epochs < max_epochs:\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
    "            summation = summation_unit(input_with_bias, weights)\n",
    "            prediction = step_function(summation)\n",
    "            error = comparator_unit(prediction, outputs[i])\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Weight update rule\n",
    "            weights += learning_rate * error * input_with_bias\n",
    "        \n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= error_threshold:\n",
    "            break\n",
    "\n",
    "    return weights, epochs, errors\n",
    "\n",
    "# Train the perceptron\n",
    "weights, epochs, errors = perceptron_training(inputs, outputs, weights, learning_rate)\n",
    "\n",
    "# Print final weights and epochs taken to converge\n",
    "print(f\"Final weights after training: {weights}\")\n",
    "print(f\"Number of epochs taken to converge: {epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e6718ef-2438-4860-aac0-796d2160ef8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: step_function, Epochs: 1000\n",
      "Activation Function: bipolar_step_function, Epochs: 1000\n",
      "Activation Function: sigmoid_function, Epochs: 1000\n",
      "Activation Function: relu_function, Epochs: 1000\n"
     ]
    }
   ],
   "source": [
    "#A3\n",
    "\n",
    "# Function to handle different activation functions\n",
    "def perceptron_training_with_activation(inputs, outputs, weights, learning_rate, activation_func, max_epochs=1000, error_threshold=0.002):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    while epochs < max_epochs:\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
    "            summation = summation_unit(input_with_bias, weights)\n",
    "            prediction = activation_func(summation)\n",
    "            error = comparator_unit(prediction, outputs[i])\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Weight update rule\n",
    "            weights += learning_rate * error * input_with_bias\n",
    "        \n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= error_threshold:\n",
    "            break\n",
    "\n",
    "    return weights, epochs, errors\n",
    "\n",
    "# Comparing different activation functions\n",
    "activation_functions = [step_function, bipolar_step_function, sigmoid_function, relu_function]\n",
    "for func in activation_functions:\n",
    "    weights, epochs, errors = perceptron_training_with_activation(inputs, outputs, weights, learning_rate, func)\n",
    "    print(f\"Activation Function: {func.__name__}, Epochs: {epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d245382f-241b-442b-bd34-142c16aac53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.1, Epochs: 1000\n",
      "Learning Rate: 0.2, Epochs: 1000\n",
      "Learning Rate: 0.30000000000000004, Epochs: 1000\n",
      "Learning Rate: 0.4, Epochs: 1000\n",
      "Learning Rate: 0.5, Epochs: 1000\n",
      "Learning Rate: 0.6000000000000001, Epochs: 1000\n",
      "Learning Rate: 0.7000000000000001, Epochs: 1000\n",
      "Learning Rate: 0.8, Epochs: 1000\n",
      "Learning Rate: 0.9, Epochs: 1000\n",
      "Learning Rate: 1.0, Epochs: 1000\n"
     ]
    }
   ],
   "source": [
    "#A4\n",
    "\n",
    "learning_rates = [0.1 * i for i in range(1, 11)]\n",
    "for lr in learning_rates:\n",
    "    weights = np.array([10, 0.2, -0.75])\n",
    "    weights, epochs, errors = perceptron_training(inputs, outputs, weights, lr)\n",
    "    print(f\"Learning Rate: {lr}, Epochs: {epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45061c14-01cc-4565-acd5-08eef233ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights for XOR gate: [110.    50.2   49.25]\n",
      "Number of epochs taken to converge for XOR gate: 1000\n"
     ]
    }
   ],
   "source": [
    "#A5\n",
    "\n",
    "# XOR Gate training data\n",
    "inputs_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Repeat training for XOR gate\n",
    "weights_xor = np.array([10, 0.2, -0.75])\n",
    "weights_xor, epochs_xor, errors_xor = perceptron_training(inputs_xor, outputs_xor, weights_xor, learning_rate)\n",
    "\n",
    "print(f\"Final weights for XOR gate: {weights_xor}\")\n",
    "print(f\"Number of epochs taken to converge for XOR gate: {epochs_xor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b72d40b-bc24-401a-8fff-8c6214c0b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [1 0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "\n",
      "MLPClassifier Weights:\n",
      "Layer 1 weights:\n",
      "[[ 0.08768621 -0.14575011 -0.18549134  0.20989438  0.08244054  0.04899746\n",
      "   0.0159367   0.00442608  0.02987509  0.01071554]\n",
      " [-1.83276218  2.04000055 -1.01081588  1.14206346 -1.89555611  1.38788268\n",
      "  -1.93214808  1.73295724  1.61591548 -1.65881009]\n",
      " [ 0.96047103 -1.29553127  0.61203513 -0.84758868  0.9909088  -0.77305325\n",
      "   0.8599837  -1.00013628 -0.891341    0.66155013]]\n",
      "Layer 2 weights:\n",
      "[[-1.48441261]\n",
      " [ 1.11669674]\n",
      " [-0.88771983]\n",
      " [ 1.44586497]\n",
      " [-1.31996287]\n",
      " [ 1.50486386]\n",
      " [-1.3326076 ]\n",
      " [ 1.27426457]\n",
      " [ 1.30293431]\n",
      " [-1.19466451]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#A6\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# A6: Load the provided dataset\n",
    "data = {\n",
    "    'Customer': ['C_1', 'C_2', 'C_3', 'C_4', 'C_5', 'C_6', 'C_7', 'C_8', 'C_9', 'C_10'],\n",
    "    'Candies': [20, 16, 27, 19, 24, 22, 15, 18, 21, 16],\n",
    "    'Mangoes': [6, 3, 6, 1, 4, 1, 4, 4, 1, 2],\n",
    "    'Milk Packets': [2, 6, 2, 2, 2, 5, 2, 2, 4, 4],\n",
    "    'Payment': [386, 289, 393, 110, 280, 167, 271, 274, 148, 198],\n",
    "    'High Value Tx?': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'High Value Tx?' to binary labels: 'Yes' -> 1, 'No' -> 0\n",
    "df['High Value Tx?'] = df['High Value Tx?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[['Candies', 'Mangoes', 'Milk Packets']]\n",
    "y = df['High Value Tx?']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='logistic', solver='adam', learning_rate_init=0.01, max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display the model weights\n",
    "print(\"\\nMLPClassifier Weights:\")\n",
    "for i, layer_weights in enumerate(mlp.coefs_):\n",
    "    print(f\"Layer {i + 1} weights:\")\n",
    "    print(layer_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "389ba53a-70d3-4e80-a6f2-6ec1479aa81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights obtained using pseudo-inverse method: [ 0.6        -0.08498457  0.47791444  0.05388368]\n",
      "Perceptron Weights: [1.         0.05513178 2.61861468 0.62254302]\n",
      "\n",
      "Confusion Matrix (Perceptron):\n",
      "[[4 0]\n",
      " [0 6]]\n",
      "\n",
      "Classification Report (Perceptron):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load the provided dataset into a DataFrame\n",
    "data = {\n",
    "    'Customer': ['C_1', 'C_2', 'C_3', 'C_4', 'C_5', 'C_6', 'C_7', 'C_8', 'C_9', 'C_10'],\n",
    "    'Candies': [20, 16, 27, 19, 24, 22, 15, 18, 21, 16],\n",
    "    'Mangoes': [6, 3, 6, 1, 4, 1, 4, 4, 1, 2],\n",
    "    'Milk Packets': [2, 6, 2, 2, 2, 5, 2, 2, 4, 4],\n",
    "    'Payment': [386, 289, 393, 110, 280, 167, 271, 274, 148, 198],\n",
    "    'High Value Tx?': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'High Value Tx?' to binary labels: 'Yes' -> 1, 'No' -> 0\n",
    "df['High Value Tx?'] = df['High Value Tx?'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Features and target variable\n",
    "features = df[['Candies', 'Mangoes', 'Milk Packets']].values\n",
    "target = df['High Value Tx?'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Function to calculate weights using the pseudo-inverse method\n",
    "def pseudo_inverse_method(X, y):\n",
    "    X_bias = np.hstack([np.ones((X.shape[0], 1)), X])  # Add a bias term\n",
    "    weights_pseudo_inverse = np.linalg.pinv(X_bias).dot(y)\n",
    "    return weights_pseudo_inverse\n",
    "\n",
    "# Calculate weights using pseudo-inverse method\n",
    "weights_pseudo_inverse = pseudo_inverse_method(features_scaled, target)\n",
    "print(f\"Weights obtained using pseudo-inverse method: {weights_pseudo_inverse}\")\n",
    "\n",
    "# Using Perceptron model for comparison\n",
    "perceptron_model = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron_model.fit(features_scaled, target)\n",
    "perceptron_weights = np.hstack([perceptron_model.intercept_, perceptron_model.coef_[0]])\n",
    "print(f\"Perceptron Weights: {perceptron_weights}\")\n",
    "\n",
    "# Predict and evaluate using perceptron model\n",
    "predictions = perceptron_model.predict(features_scaled)\n",
    "print(\"\\nConfusion Matrix (Perceptron):\")\n",
    "print(confusion_matrix(target, predictions))\n",
    "print(\"\\nClassification Report (Perceptron):\")\n",
    "print(classification_report(target, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fde17443-8a92-4b59-ba64-5fa0628a8494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weights after training: [[1.02164576 1.03950672]\n",
      " [0.42067785 0.75463218]\n",
      " [0.29363162 0.47960077]]\n",
      "Final output weights after training: [2.47513516 1.94746188 2.32015979]\n",
      "Number of epochs taken to converge: 1000\n"
     ]
    }
   ],
   "source": [
    "#A8\n",
    "\n",
    "# A8: Neural Network with Backpropagation\n",
    "\n",
    "# Initialize weights for the neural network\n",
    "weights_hidden = np.random.rand(3, 2)  # Weights for 2 neurons in the hidden layer (including bias)\n",
    "weights_output = np.random.rand(3)  # Weights for the output layer (including bias)\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Sigmoid Activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Backpropagation algorithm for a single hidden layer neural network\n",
    "def backpropagation_and_gate(inputs, outputs, weights_hidden, weights_output, learning_rate, max_epochs=1000, error_threshold=0.002):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            # Forward pass\n",
    "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
    "            hidden_layer_input = np.dot(input_with_bias, weights_hidden)\n",
    "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "            hidden_output_with_bias = np.insert(hidden_layer_output, 0, 1)  # Adding bias for hidden to output layer\n",
    "            final_input = np.dot(hidden_output_with_bias, weights_output)\n",
    "            final_output = sigmoid(final_input)\n",
    "\n",
    "            # Calculate error\n",
    "            error = comparator_unit(final_output, outputs[i])\n",
    "            total_error += error ** 2\n",
    "\n",
    "            # Backward pass (Error Backpropagation)\n",
    "            delta_output = error * sigmoid_derivative(final_output)\n",
    "            delta_hidden = delta_output * weights_output[1:] * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "            # Update weights\n",
    "            weights_output += learning_rate * delta_output * hidden_output_with_bias\n",
    "            weights_hidden += learning_rate * np.outer(input_with_bias, delta_hidden)\n",
    "\n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= error_threshold:\n",
    "            break\n",
    "\n",
    "    return weights_hidden, weights_output, epochs, errors\n",
    "\n",
    "# Train the neural network using backpropagation for AND gate\n",
    "weights_hidden, weights_output, epochs, errors = backpropagation_and_gate(inputs, outputs, weights_hidden, weights_output, learning_rate)\n",
    "\n",
    "# Print final weights and epochs taken to converge\n",
    "print(f\"Final hidden weights after training: {weights_hidden}\")\n",
    "print(f\"Final output weights after training: {weights_output}\")\n",
    "print(f\"Number of epochs taken to converge: {epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a648dbe-59c0-4ec8-bda4-d6bbe31fb524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weights after training for XOR gate:\n",
      "[[0.90571962 7.32475086]\n",
      " [0.90572643 7.32731531]]\n",
      "Final output weights after training for XOR gate:\n",
      "[[-27.46354604]\n",
      " [ 21.74964594]]\n",
      "Number of epochs taken to converge for XOR gate: 9999\n",
      "\n",
      "Predictions after training:\n",
      "[[0.05432317]\n",
      " [0.89824588]\n",
      " [0.89824604]\n",
      " [0.13513664]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Backpropagation function for XOR gate\n",
    "def backpropagation_xor(inputs, outputs, weights_hidden, weights_output, learning_rate, epochs=10000):\n",
    "    # Initialize error tracking\n",
    "    errors = []\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        hidden_input = np.dot(inputs, weights_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        final_input = np.dot(hidden_output, weights_output)\n",
    "        final_output = sigmoid(final_input)\n",
    "\n",
    "        # Calculate error (difference between expected and predicted outputs)\n",
    "        error = outputs - final_output\n",
    "        errors.append(np.mean(np.abs(error)))\n",
    "\n",
    "        # Backpropagation\n",
    "        output_delta = error * sigmoid_derivative(final_output)\n",
    "        hidden_error = output_delta.dot(weights_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(hidden_output)\n",
    "\n",
    "        # Update weights\n",
    "        weights_output += hidden_output.T.dot(output_delta) * learning_rate\n",
    "        weights_hidden += inputs.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "    return weights_hidden, weights_output, epoch, errors\n",
    "\n",
    "# XOR gate training data\n",
    "inputs_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights (randomly)\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "weights_hidden = np.random.uniform(size=(2, 2))  # 2 input nodes, 2 hidden nodes\n",
    "weights_output = np.random.uniform(size=(2, 1))  # 2 hidden nodes, 1 output node\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Train the neural network using backpropagation for XOR gate\n",
    "weights_hidden_xor, weights_output_xor, epochs_xor, errors_xor = backpropagation_xor(\n",
    "    inputs_xor, outputs_xor, weights_hidden, weights_output, learning_rate\n",
    ")\n",
    "\n",
    "# Print final weights and epochs taken to converge\n",
    "print(f\"Final hidden weights after training for XOR gate:\\n{weights_hidden_xor}\")\n",
    "print(f\"Final output weights after training for XOR gate:\\n{weights_output_xor}\")\n",
    "print(f\"Number of epochs taken to converge for XOR gate: {epochs_xor}\")\n",
    "\n",
    "# Test the network with XOR inputs\n",
    "hidden_output_test = sigmoid(np.dot(inputs_xor, weights_hidden_xor))\n",
    "final_output_test = sigmoid(np.dot(hidden_output_test, weights_output_xor))\n",
    "print(\"\\nPredictions after training:\")\n",
    "print(final_output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea8c4414-c2df-4605-ba04-86e046e08436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weights after training with 2 output nodes:\n",
      "[[ 5.22052764 -0.99764253 -1.96946528]\n",
      " [-3.84821518  0.8727374   1.75448529]\n",
      " [-3.51663571  1.71548308  1.65824936]]\n",
      "Final output weights after training with 2 output nodes:\n",
      "[[ 0.59939072 -0.84301645]\n",
      " [ 6.25342035 -6.01860498]\n",
      " [-2.2602043   2.24773264]\n",
      " [-2.75740899  2.98994899]]\n",
      "Number of epochs taken to converge with 2 output nodes: 1000\n",
      "Input: [0 0] => Output: [0.9971925  0.00284992]\n",
      "Input: [0 1] => Output: [0.96102249 0.04069767]\n",
      "Input: [1 0] => Output: [0.96433906 0.03712207]\n",
      "Input: [1 1] => Output: [0.05452448 0.94330969]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Modified backpropagation function for neural networks with 2 output nodes\n",
    "def backpropagation_two_output_nodes(inputs, outputs, weights_hidden, weights_output, learning_rate, max_epochs=1000, error_threshold=0.002):\n",
    "    epochs = 0\n",
    "    errors = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            # Forward pass\n",
    "            input_with_bias = np.insert(inputs[i], 0, 1)  # Adding bias input (1)\n",
    "            hidden_layer_input = np.dot(input_with_bias, weights_hidden)\n",
    "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "            hidden_output_with_bias = np.insert(hidden_layer_output, 0, 1)  # Adding bias for hidden to output layer\n",
    "            final_input = np.dot(hidden_output_with_bias, weights_output)\n",
    "            final_output = sigmoid(final_input)\n",
    "\n",
    "            # Calculate error\n",
    "            error = outputs[i] - final_output\n",
    "            total_error += np.sum(error ** 2)\n",
    "\n",
    "            # Backward pass (Error Backpropagation)\n",
    "            delta_output = error * sigmoid_derivative(final_output)\n",
    "            delta_hidden = np.dot(weights_output[1:], delta_output) * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "            # Update weights\n",
    "            weights_output += learning_rate * np.outer(hidden_output_with_bias, delta_output)\n",
    "            weights_hidden += learning_rate * np.outer(input_with_bias, delta_hidden)\n",
    "\n",
    "        errors.append(total_error)\n",
    "        epochs += 1\n",
    "\n",
    "        if total_error <= error_threshold:\n",
    "            break\n",
    "\n",
    "    return weights_hidden, weights_output, epochs, errors\n",
    "\n",
    "# AND gate training data with two output nodes (one-hot encoded)\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs for AND gate\n",
    "outputs_and_two_nodes = np.array([[1, 0], [1, 0], [1, 0], [0, 1]])  # Mapping 0 to [1, 0] and 1 to [0, 1]\n",
    "\n",
    "# Initialize weights (randomly)\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "weights_hidden = np.random.rand(3, 3)  # 3 input nodes (including bias), 3 hidden nodes\n",
    "weights_output_two_nodes = np.random.rand(4, 2)  # 3 hidden nodes + 1 bias, 2 output nodes\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Train the neural network with 2 output nodes for AND gate\n",
    "weights_hidden_two_nodes, weights_output_two_nodes, epochs_two_nodes, errors_two_nodes = backpropagation_two_output_nodes(\n",
    "    inputs, outputs_and_two_nodes, weights_hidden, weights_output_two_nodes, learning_rate\n",
    ")\n",
    "\n",
    "# Print final weights and epochs taken to converge\n",
    "print(f\"Final hidden weights after training with 2 output nodes:\\n{weights_hidden_two_nodes}\")\n",
    "print(f\"Final output weights after training with 2 output nodes:\\n{weights_output_two_nodes}\")\n",
    "print(f\"Number of epochs taken to converge with 2 output nodes: {epochs_two_nodes}\")\n",
    "\n",
    "# Test the network with AND inputs\n",
    "for input_data in inputs:\n",
    "    input_with_bias = np.insert(input_data, 0, 1)  # Add bias to the input\n",
    "    hidden_layer_output = sigmoid(np.dot(input_with_bias, weights_hidden_two_nodes))\n",
    "    hidden_output_with_bias = np.insert(hidden_layer_output, 0, 1)  # Add bias to hidden layer output\n",
    "    final_output = sigmoid(np.dot(hidden_output_with_bias, weights_output_two_nodes))\n",
    "    print(f\"Input: {input_data} => Output: {final_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdb98de9-b482-4c01-83ea-3ccb461d3d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier Weights for AND gate: [array([[-4.66439058, -4.70971052],\n",
      "       [-4.70187354, -4.6722469 ]]), array([[-5.52916835],\n",
      "       [-4.94161041]])]\n",
      "MLPClassifier Weights for XOR gate: [array([[-7.21792739, -6.93529282],\n",
      "       [-4.67000142,  4.70247249]]), array([[-5.92964435],\n",
      "       [ 6.08290793]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1105: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#A11\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# A11: Using MLPClassifier for AND Gate\n",
    "mlp_and = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', learning_rate_init=0.05, max_iter=1000)\n",
    "mlp_and.fit(inputs, outputs)\n",
    "print(f\"MLPClassifier Weights for AND gate: {mlp_and.coefs_}\")\n",
    "\n",
    "# A11: Using MLPClassifier for XOR Gate\n",
    "mlp_xor = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', learning_rate_init=0.05, max_iter=1000)\n",
    "mlp_xor.fit(inputs_xor, outputs_xor)\n",
    "print(f\"MLPClassifier Weights for XOR gate: {mlp_xor.coefs_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86ba53a4-893d-483b-bd14-c9d2ee77477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0 ...  0  0  0]\n",
      " [ 0 13  0 ...  0  0  0]\n",
      " [ 0  0 15 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 19  0  0]\n",
      " [ 0  0  0 ...  0 10  0]\n",
      " [ 0  0  0 ...  0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        3333       1.00      0.87      0.93        15\n",
      "        3334       1.00      1.00      1.00        13\n",
      "        3335       1.00      1.00      1.00        15\n",
      "        3337       0.88      1.00      0.93         7\n",
      "        3342       1.00      0.91      0.95        22\n",
      "        3343       0.00      0.00      0.00         1\n",
      "        3346       0.86      1.00      0.92        12\n",
      "        3349       0.96      0.87      0.91        30\n",
      "        3350       0.00      0.00      0.00         1\n",
      "        3351       0.13      0.30      0.18        10\n",
      "        3352       0.12      0.05      0.07        19\n",
      "        3353       1.00      1.00      1.00        19\n",
      "        3354       0.96      1.00      0.98        22\n",
      "        3355       0.00      0.00      0.00         1\n",
      "        3356       0.93      0.93      0.93        14\n",
      "        3357       0.00      0.00      0.00         1\n",
      "        3358       0.00      0.00      0.00         1\n",
      "        3359       0.95      0.84      0.89        25\n",
      "        3360       0.82      1.00      0.90         9\n",
      "        3361       1.00      1.00      1.00         5\n",
      "        3362       1.00      1.00      1.00         5\n",
      "        3363       1.00      1.00      1.00        13\n",
      "        3364       0.82      0.82      0.82        11\n",
      "        3365       0.67      0.67      0.67         6\n",
      "        3366       1.00      0.94      0.97        16\n",
      "        3367       0.92      1.00      0.96        12\n",
      "        3368       0.92      0.85      0.88        26\n",
      "        3370       0.59      0.83      0.69        12\n",
      "        3371       1.00      1.00      1.00         6\n",
      "        3372       1.00      1.00      1.00         9\n",
      "        3373       0.94      1.00      0.97        16\n",
      "        3374       0.96      0.86      0.91        29\n",
      "        3375       0.89      1.00      0.94        17\n",
      "        3376       0.88      0.78      0.82        27\n",
      "        3377       1.00      0.91      0.95        22\n",
      "        3378       0.91      0.91      0.91        22\n",
      "        3379       0.85      0.85      0.85        13\n",
      "        3380       1.00      1.00      1.00         4\n",
      "        3381       0.82      0.96      0.88        24\n",
      "        3382       1.00      1.00      1.00        16\n",
      "        3383       1.00      1.00      1.00        16\n",
      "        3384       0.89      0.81      0.85        21\n",
      "        3385       1.00      1.00      1.00        22\n",
      "        3450       1.00      1.00      1.00         7\n",
      "        3451       0.85      1.00      0.92        17\n",
      "        3452       0.79      0.90      0.84        21\n",
      "        3453       0.91      1.00      0.95        10\n",
      "        3454       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.89       675\n",
      "   macro avg       0.80      0.81      0.80       675\n",
      "weighted avg       0.89      0.89      0.88       675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\yuvra\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#A12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:/DCT_withoutduplicate 3 (1).csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the last column is the target and the rest are features\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (important for MLP)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, activation='relu', solver='adam', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd5711-e318-4b37-a3ea-5686b033f8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
